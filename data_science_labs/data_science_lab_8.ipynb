{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvidLkZCc4A2"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porterjenkins/CS180/blob/main/data_science_labs/data_science_lab_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0NimIjkdTzz"
      },
      "source": [
        "# BYU CS 180 Lab 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHbfWVe-dMYz"
      },
      "source": [
        "## Introduction:\n",
        "For this assignment, you will build a simple univariate linear regression model to predict profits for a food truck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NewZD37dbrM"
      },
      "source": [
        "Download the data that you will use by running this command, and read it in using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbrM8d_3b91Y",
        "outputId": "44631617-aa50-48bb-cff7-b0d0d27c84b6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "!wget https://raw.githubusercontent.com/porterjenkins/cs180-intro-data-science/master/data/food-truck.csv\n",
        "data = pd.read_csv(\"food-truck.csv\", header=None, names=['X', 'Y'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h5eqHpQ9yx7Q"
      },
      "source": [
        "We will restrict ourselves to a linear hypothesis space, constructing a model that adheres to the following form:\n",
        "$$ f _\\Theta(x) = \\theta _0 + \\theta _1x $$\n",
        "\n",
        "You might notice that this equation is similar to the linear equation: $$ f(x) = b + mx $$\n",
        "\n",
        "(Yes you did use y=mx+b after 8th grade ðŸ˜œ)\n",
        "\n",
        "\n",
        "In this lab, you will be writing a machine learning model that learns/approximates 2 parameters. The first one is $\\theta_0$ which represents learning *b* (the bias or the intercept). The second one is $\\theta_1$ which represents learning *m* (the weight, or the slope). This type of machine learning model is traditionally called Least Squares model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDiIwcCh1Owd"
      },
      "source": [
        "Given data, your goal will be to estimate the parameters of this model using the method of steepest gradient descent. The parameters are defined within the construct:\n",
        "$$ \\theta_p = \\{\\theta_0,\\theta_1\\} $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jc-wWrUc12NG"
      },
      "source": [
        "which is the vector of learnable coefficients that weight the observed variables, and where $\\theta_1$ is a single bias coefficient. We can learn these parameters by minimizing average squared error. Thus, the loss function you will want to implement is:\n",
        "\n",
        "Equation 1: $$ L(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m (f(x^i)-y^i)^2 $$\n",
        "\n",
        "This equation may look very scary at first, but it's really not that scary, so let's break it down, and define our variables.\n",
        "*   m is the number of datapoints in the data set.\n",
        "*   i is the index of the data point tuple that we're looking at in the sum.\n",
        "*   L is the loss function (think of it like f(x)).\n",
        "*   $\\Theta$ is the list of parameters to estimate.\n",
        "*   $(f(x^i)-y^i)^2$ is the squared difference between the predicted output and the actual output.\n",
        "\n",
        "So wrapping it all together, the loss function is taking the sum of the squared differences, and dividing it by double the number of datapoints in the dataset.\n",
        "\n",
        "The goal of this lab is to minimize the loss function, because that means that our predicted values and the actual values are getting closer together. (Quick thought exercise, ask yourself why we want our predicted values to get closer to the actual values)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LiYPG03jKvWQ"
      },
      "source": [
        "## Exercise 1: The Dark Descent (Compute Loss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6q-MueNuPh"
      },
      "source": [
        "In order to correctly recreate the gradient descent algorithm, you need to compute the loss function. Use the equation for loss given in the introduction to fill out the functions below to correctly produce the right loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVDwIWcJOegx"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "  # TODO: Write the code to output the predicted y values based on the X values\n",
        "  # Hint: Remember that theta is a tuple consisting of theta_0 and theta_1\n",
        "  y_predictions = \n",
        "  return y_predictions\n",
        "\n",
        "def sum_mean_squared_error(y, y_hat):\n",
        "  # TODO: Write the sum of the mean squared error\n",
        "  # Hint: Follow the loss equation\n",
        "  mean_squared_error = \n",
        "  return mean_squared_error\n",
        "\n",
        "def calculate_loss(X, y, theta):\n",
        "\t# TODO: Write your compute loss function below\n",
        "  # Hint: You'll use the predict and the sum_mean_squared_error functions defined above\n",
        "  y_hat = \n",
        "  loss = \n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAUhDKl1QkxU"
      },
      "source": [
        "Explain to me like I'm five what the code above is doing:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc2Dc_tzQsLz"
      },
      "source": [
        ">(Enter Answer Here)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsatAR4GGUq"
      },
      "source": [
        "## Exercise 2: Partial Derivatives"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5zGn1zlbGK56"
      },
      "source": [
        "Analytically derive the gradient of the loss function with respect to the model parameters, $\\theta_0$ and $\\theta_1$. (take the partial derivative with respect to the given parameters)\n",
        "\n",
        "Hint: You will need to find these two derivatives in order to calculate the gradient (L is just a less fancy version of the L(Î¸) above):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye_vjnFKGiR-"
      },
      "source": [
        "Give the partial derivative for the parameter $\\theta_0$ below:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Csf84DuKGw_o"
      },
      "source": [
        ">(Enter Answer Here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Dg07RwGrgn"
      },
      "source": [
        "Give the partial derivative for the parameter $\\theta_1$ below:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jvD2WWiWKid0"
      },
      "source": [
        ">(Enter Answer Here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C82MvdtJK-si"
      },
      "source": [
        "Using the two equations above, fill out the gradient calculation function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INIE_kqpK7sm"
      },
      "outputs": [],
      "source": [
        "def calculate_gradient(X,y, theta):\n",
        "  # TODO: The gradient with respect to the bias (the y intercept)\n",
        "  dL_d0 = \n",
        "  # TODO: The gradient with respect to the weight (the slope)\n",
        "  dL_d1 = \n",
        "  \n",
        "  # nabla represents the full gradient, or a vector of the partial derivatives\n",
        "  nabla = (dL_d0, dL_d1)\n",
        "  return nabla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3P97BfsQ3JB"
      },
      "source": [
        "## Exercise 3: Hold The Line (Training Your Least Squared Algorithm)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IY0BRQMpRlC-"
      },
      "source": [
        "### There are a couple of hyperparameters that you should know about before training your algorithm:\n",
        "\n",
        "#### Epochs / Iterations:\n",
        "Epochs is the number of times that the algorithm runs through the dataset. For example, if the epoch count is 500, then the algorithm with iterate through the dataset 500 times.\n",
        "\n",
        "#### Learning Rate / Step Size:\n",
        "Learning rate (traditionally denoted as Î± or alpha) is the rate at which your algorithm learns. You can also think of it as the step size for updating the weight and bias ($\\theta_0$ and $\\theta_1$). If the step size is too large, the loss value will blow up and never converge. If the learning rate is too small, the algorithm will take forever to converge, and won't be an efficient use of your time. If you have a smaller learning rate, I recommend using a higher epoch count.\n",
        "\n",
        "#### Batch Size:\n",
        "You won't use batch size for this lab, but I still feel like it's an important concept to learn. Batch size is splitting the dataset up into smaller chunks and then feeding those smaller chuncks of data into the ML model. When it comes to larger models such as Neural Networks, it's important to have the right batch size otherwise your computer may run out of memory.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now that you understand the hyperparameters, let's train our model!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JUtKOGZOGxqF"
      },
      "source": [
        "You will update the current value for each theta with the function:\n",
        "\n",
        "Equation 2:\n",
        "$$\\theta_n = \\theta_p - Î± \\frac{dL(\\Theta)}{d\\theta_p}$$\n",
        "\n",
        "Again, there are some scary letters, but that's not to worry, I'm going to break them down for you below.\n",
        "\n",
        "* $\\theta_n$ is the updated parameter tuple.\n",
        "* $\\theta_p$ is the previous parameter tuple.\n",
        "* Î± is the learning rate. (I recommend a small value for this, like 0.1, 0.01, or 0.001. Once the lab is finished you should try different values for your learning rate and see how it changes the convergence of the parameters).\n",
        "* $\\frac{dL(\\Theta)}{d\\theta_p}$ is the gradient (vector of partial derviatives) that you'll be updating.\n",
        "\n",
        "Putting it all together, we have the previous parameters being subtracted from the gradient multiplied by a learning rate.\n",
        "\n",
        "Attempt to explain why/how this function uses the loss function minimize the parameters. (Credit will be given at an attempt of an explanation, not based on correctness):"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sdCxSZAcKsS9"
      },
      "source": [
        ">(Enter Answer Here)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxx63wSgZVLJ"
      },
      "source": [
        "Fill out the TODO parts of the training function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EFH9H1cRhSX"
      },
      "outputs": [],
      "source": [
        "def train_model(dataframe, epochs, learning_rate):\n",
        "  # Get the X and y values of the column\n",
        "  X = dataframe.iloc[:, 0]\n",
        "  y = dataframe.iloc[:, 1]\n",
        "\n",
        "  # Turn the X and y values into numpy arrays\n",
        "  X = np.array(X.values)\n",
        "  y = np.array(y.values)\n",
        "\n",
        "  # convert to numpy arrays and initalize the parameter array theta\n",
        "  w = np.zeros(1)\n",
        "  b = np.zeros(1)\n",
        "  theta = (b, w)\n",
        "\n",
        "  # TODO: calculate the initial loss\n",
        "  initial_L = \n",
        "\n",
        "  # Initialize the list of loss values\n",
        "  loss_values = [initial_L]\n",
        "\n",
        "  # Initialize I to 0\n",
        "  i = 0\n",
        "\n",
        "  while i < epochs:\n",
        "    # TODO: Calculate Gradient\n",
        "    dL_db, dL_dw = \n",
        "    t_0 = theta[0]\n",
        "    t_1 = theta[1]\n",
        "\n",
        "    # TODO: update theta with respect to the calculated gradient\n",
        "    # Hint: use the equation above\n",
        "    updated_t_0 = \n",
        "    updated_t_1 = \n",
        "\n",
        "    theta = (updated_t_0, updated_t_1)\n",
        "\n",
        "    # TODO: Calculated new loss using the updated theta values, and add it to the loss_values list\n",
        "    L = \n",
        "\n",
        "    # Update I\n",
        "    i += 1\n",
        "    \n",
        "  return (loss_values, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sv8tD_aYaPK"
      },
      "outputs": [],
      "source": [
        "def plot_line(dataframe, theta, epoch, learning_rate):\n",
        "  # This function will plot your model based on some sampled values\n",
        "  # There is no need to change this function, you'll just need to call it\n",
        "  \n",
        "  X = dataframe.iloc[:, 0]\n",
        "  y = dataframe.iloc[:, 1]\n",
        "\n",
        "  kludge = 0.25\n",
        "  X_test = np.linspace(data.X.min(), data.X.max(), 100)\n",
        "  X_test = np.expand_dims(X_test, axis=1)\n",
        "  \n",
        "  plt.plot(X_test, predict(X_test, theta), label=\"Model\")\n",
        "  plt.title(f\"Value for {epoch} epochs and {learning_rate} step\")\n",
        "  plt.scatter(X, y, edgecolor='g', s=20, label=\"Samples\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.xlim((np.amin(X_test) - kludge, np.amax(X_test) + kludge))\n",
        "  plt.ylim((np.amin(y) - kludge, np.amax(y) + kludge))\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oYajIvGMZr8j"
      },
      "source": [
        "Use the two functions above to do plot the models after 5, 100, 1000, and 10,000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsijuvK4ZSCZ"
      },
      "outputs": [],
      "source": [
        "# Write the code to plot the line for the following cases:\n",
        "# 5 Epochs\n",
        "\n",
        "# 100 Epochs\n",
        "\n",
        "# 1000 Epochs\n",
        "\n",
        "# 10,000 Epochs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9UuayB93aVOJ"
      },
      "source": [
        "Using the results after 10,000 epochs, plot the loss values with respect to epochs using a line graph (epochs on the x axis and loss values on the y axis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7d77l9HaTEo"
      },
      "outputs": [],
      "source": [
        "# Write the code to plot the loss values for each of the 10,000 iterations:\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke029wcPasAb"
      },
      "source": [
        "## Exercise 4: Discussion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3yT8gXHhau04"
      },
      "source": [
        "Since learning a linear model is a convex optimization problem, you should see the model converge to a low mean squared error. However, you will need to tune the learning rate/step size, Î± (bear in mind that values that are too big will result in divergence).\n",
        "Write a few sentences describing what you learned from the training/model fitting process. \n",
        "\n",
        "Things to discuss: What happens when you change the step size Î±? How many epochs did you need to converge on a reasonable solution (for any given step size)?\n",
        "\n",
        "You should also discuss the hyperparameters you tried, and which ones worked best. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tKUndHnBa1C0"
      },
      "source": [
        ">(Enter Answer Here)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "921445e9336842bf3da7ce0cce684b71b836ed2d0c3aed21b6777cfb0e4f3429"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
